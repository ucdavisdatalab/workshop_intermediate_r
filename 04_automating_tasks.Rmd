Reshaping Data & Automating Tasks
=================================

This chapter is part 2 (of 2) of _Cleaning Data & Automating Tasks_, a workshop
series about how to prepare data for analysis and automate tedious repetitive
tasks. The major topics of this chapter are how to reshape datasets with
pivots, how to combine related datasets with joins, and how to select and use
iteration strategies that automate repetitive computations.


#### Learning Objectives {-}

* Explain what it means for data to be tidy
* Use the tidyr package to reshape data
* Describe and use R's for, while, and repeat loops
* Identify the most appropriate iteration strategy for a given problem
* Explain strategies to organize iterative code



Tidy Datasets
-------------

The structure of a dataset---its shape and organization---has enormous
influence on how difficult it will be to analyze, so making structural changes
is an important part of the cleaning process. Researchers conventionally
arrange tabular datasets so that each row contains a single **observation** or
case, and each column contains a single kind of measurement or identifier,
called a **feature**.

In 2014, [Hadley Wickham][hadley] refined and formalized the conventions for
tabular datasets by [introducing the concept of **tidy datasets**][tidy], which
have a specific structure. Paraphrasing Wickham, the rules for a tidy dataset
are:

[hadley]: https://hadley.nz/
[tidy]: https://vita.had.co.nz/papers/tidy-data.html

> 1. Every column is a single feature.
> 2. Every row is a single observation.
> 3. Every cell is a single value.

These rules ensure that all of the values in a dataset are visually organized
and are easy to access with indexing operations. They're also specific enough
to make tidiness a convenient standard for functions that operate on tabular
datasets. In fact, the [Tidyverse][tidyverse] packages (see Section
\@ref(the-tidyverse)) are designed from the ground up for working with tidy
datasets. Tidy datesets have also been adopted as a standard in other software,
including various packages for Python and Julia.

[tidyverse]: https://www.tidyverse.org/

This section explains how to **reshape** tabular datasets into tidy datasets.
While reshaping can seem tricky at first, making sure your dataset has the
right structure before you begin analysis saves time and frustration in the
long run.

<!--
The Tidyverse is so named because many functions in Tidyverse packages require
data frames that are in **tidy** form. Before we see the requirements for a
data set to be tidy, we need to define or review some terminology from
statistics.

A **feature** (also called a **covariate** or a **variable**) is a measurement
of something, usually across multiple subjects. For example, we might decide to
measure the heights of everyone in the class. Each person in the class is a
subject, and the height measurement is a feature. Features don't have to be
quantitative. If we also asked each person their favorite color, then favorite
color would be another feature in our data set. Features are usually, but not
always, the columns in a tabular data set.

An **observation** is a set of features measured for a single subject or at a
single time. So in the preceding example, the combined height and favorite
color measurement for one student is one observation. Observations are usually,
but not always, the rows in a tabular data set.
-->


### The tidyr Package

The [tidyr][] package provides functions to reshape tabular datasets. It also
provides examples of tidy and untidy datasets. Like most Tidyverse packages, it
comes with detailed [documentation][tidyr] and a [cheatsheet][tidyr-cheat].

[tidyr]: https://tidyr.tidyverse.org/
[tidyr-cheat]: https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf

As usual, install the package if you haven't already, and then load it:

```{r}
# install.packages("tidyr")
library(tidyr)
```

Let's start with an example of a tidy dataset. The `table1` dataset in the
package records the number of tuberculosis cases across several different
countries and years:

```{r}
table1
```

Each of the four columns contains a single kind of measurement or identifier,
so the dataset satifies tidy rule 1. The measurements were taken at the
country-year level, and each row contains data for one country-year pair, so
the dataset also satisfies tidy rule 2. Each cell in the data frame only
contains one value, so the dataset also satisfies tidy rule 3.

The same data are recorded in `table2`, `table3`, and the pair `table4a` with
`table4b`, but these are all *untidy* datasets. For example, `table2` breaks
rule 1 because the column `count` contains two different kinds of
measurements---case counts and population counts:

```{r}
table2
```

When considering whether you should reshape a dataset, think about what the
features are and what the observations are. These depend on the dataset itself,
but also on what kinds of analyses you want to do. Datasets sometimes have
closely related features or multiple (nested) levels of observation. The tidyr
documentation includes a [detailed article on how to reason about reshaping
datasets][tidyr-tidy].

[tidyr-tidy]: https://tidyr.tidyverse.org/articles/tidy-data.html

If you do decide to reshape a dataset, then you should also think about what
role each feature serves:

* **Identifiers** are labels that distinguish observations from one another.
  They are often but not always categorical. Examples include names or
  identification numbers, treatment groups, and dates or times. In the
  tuberculosis data set, the `country` and `year` columns are identifiers.

* **Measurements** are the values collected for each observation and typically
  the values of research interest. For the tuberculosis data set, the `cases`
  and `population` columns are measurements.

Having a clear understanding of which features are identifiers and which are
measurements makes it easier to use the tidyr functions.


### Rows into Columns

Tidy data rule 1 is that each column must be a single feature. The `table2`
dataset breaks this rule:

```{r}
table2
```

To make the dataset tidy, the measurements in the `count` column need to be
separated into two separate columns, `cases` and `population`, based on the
categories in the `type` column.

You can use the `pivot_wider` function to **pivot** the single `count` column
into two columns according to the `type` column. This makes the dataset wider,
hence the name `pivot_wider`.

The function's first parameter is the dataset to pivot. Other important
parameters are:

* `values_from` -- The column(s) to pivot.
* `names_from` -- The column that contains names for the new columns.
* `id_cols` -- The identifier columns, which are not pivoted. This defaults to
  all columns except those in `values_from` and `names_from`.

Here's how to use the function to make `table2` tidy:

```{r}
pivot_wider(table2, values_from = count, names_from = type)
```

The function automatically removes values from the `country` and `year` columns
as needed to maintain their original correspondence with the pivoted values.


### Columns into Rows

Tidy data rule 2 is that every row must be a single observation. The `table4a`
and `table4b` datasets break this rule because each row in each dataset
contains measurements for two different years:

```{r}
table4a
table4b
```

The tuberculosis case counts are in `table4a`. The population counts are in
`table4b`. Neither is tidy.

To make the `table4a` dataset tidy, the `1999` and `2000` columns need to be
pivoted into two new columns: one for the measurements (the counts) and one for
the identifiers (the years). It might help to visualize this as stacking the
two separate columns `1999` and `2000` together, one on top of the other, and
then adding a second column with the appropriate years. The same process makes
`table4b` tidy.

You can use the `pivot_longer` function to pivot the two columns `1999` and
`2000` into a column of counts and a column of years. This makes the dataset
longer, hence the name `pivot_longer`.

Again the function's first parameter is the dataset to pivot. Other important
parameters are:

* `cols` -- The columns to pivot.
* `values_to` -- Name(s) for the new measurement column(s)
* `names_to` -- Name(s) for the new identifier column(s)

Here's how to use the function to make `table4a` tidy:

```{r}
tidy4a = pivot_longer(table4a, -country, values_to = "cases",
  names_to = "year")
tidy4a
```

In this case, the `cols` parameter is set to all columns *except* the `country`
column, because the `country` column does not need to be pivoted. The function
automatically repeats values in the `country` column as needed to maintain its
original correspondence with the pivoted values.

Here's the same for `table4b`:

```{r}
tidy4b = pivot_longer(table4b, -country, values_to = "population",
  names_to = "year")
tidy4b
```

Once the two datasets are tidy, you can join them with the `merge` function to
reproduce `table1`:

```{r}
merge(tidy4a, tidy4b)
```


### Separating Values

Tidy data rule 3 says each value must have its own cell. The `table3` dataset
breaks this rule because the `rate` column contains two values per cell:

```{r}
table3
```

The two values separated by `/` in the `rate` column are the tuberculosis case
count and the population count.

To make this dataset tidy, the `rate` column needs to be split into two
columns, `cases` and `population`. The values in the `rate` column are strings,
so one way to do this is with the stringr package's `str_split_fixed` function,
described in Section \@ref(splitting-strings):

```{r}
library(stringr)

# Split the rate column into 2 columns.
cols = str_split_fixed(table3$rate, fixed("/"), 2)

# Remove the rate column and append the 2 new columns.
tidy3 = table3[-3]
tidy3$cases = as.numeric(cols[, 1])
tidy3$population = as.numeric(cols[, 2])
tidy3
```

Extracting values, converting to appropriate data types, and then combining
everything back into a single data frame is an extremely common pattern in data
science.

The tidyr package provides the `separate` function to streamline the steps
taken above. The first parameter is the dataset, the second is the column to
split, the third is the names of the new columns, and the fourth is the
delimiter. The `convert` parameter controls whether the new columns are
automatically converted to appropriate data types:

```{r}
separate(table3, rate, c("cases", "population"), "/", convert = TRUE)
```

As of writing, the tidyr developers have deprecated the `separate` function in
favor of several more specific functions (`separate_wider_delim`,
`separate_wider_position`, and `separate_wider_regex`). These functions are
still experimental, so we still recommend using the `separate` function in the
short term.


### Case Study: SMART Ridership

[Sonoma-Marin Area Rail Transit (SMART)][smart] is a single-line passenger rail
service between the San Francisco Bay and Santa Rosa. They publish [data about
monthly ridership][smart-riders] in PDF and Excel format. In this case study,
you'll reshape and clean the dataset to prepare it for analysis.

[smart]: http://sonomamarintrain.org/
[smart-riders]: https://www.sonomamarintrain.org/RidershipReports

To get started, download the [December 2022 report it Excel
format][smart-dec22]. Pay attention to where you save the file---or move it to
a directory just for files related to this case study---so that you can load it
into R. If you want, you can use R's `download.file` function to download the
file rather than your browser.

The [readxl][] package provides functions to read data from Excel files.
Install the package if you don't already have it installed, and then load it:

[smart-dec22]: https://www.sonomamarintrain.org/sites/default/files/Ridership%20Reports/SMART%20Ridership%20Web%20Posting_Dec.22.xlsx
[readxl]: https://readxl.tidyverse.org/

```{r load-readxl}
# install.packages("readxl")
library("readxl")
```

You can use the `read_excel` function to read a sheet from an Excel
spreadsheet. Before doing so, it's a good idea to manually inspect the
spreadsheet in a spreadsheet program. The SMART dataset contains two tables in
the first sheet, one for total monthly ridership and another for average
weekday ridership (by month).

Let's focus on the total monthly ridership table, which occupies cells B4 to
H16. You can specify a range of cells when you call `read_excel` by setting the
`range` parameter:

```{r read-smart-data}
smart_path = "./data/SMART Ridership Web Posting_Dec.22.xlsx"
smart = read_excel(smart_path, range = "B4:H16")
smart
```

The loaded dataset needs to be cleaned. The `FY18` column uses a hyphen to
indicate missing data and has the wrong data type. The dates---months and
years---are identifiers for observations, so the dataset is also not tidy.

You can correct the missing value in the `FY18` column with indexing, and the
type with the `as.numeric` function:

```{r}
smart$FY18[smart$FY18 == "-"] = NA
smart$FY18 = as.numeric(smart$FY18)
head(smart)
```

To make the dataset tidy, it needs to be reshaped so that the values in the
various fiscal year columns are all in one column. In other words, the dataset
needs to be pivoted longer (Section \@ref(columns-into-rows)). The result of
the pivot will be easier to understand if you rename the columns as their years
first. Here's one way to do that:

```{r}
names(smart)[-1] = 2018:2023
head(smart)
```

Next, use `pivot_longer` to pivot the dataset:

```{r pivot-smart-data}
smart = pivot_longer(smart, -Month, values_to = "riders",
  names_to = "fiscal_year")
head(smart)
```

Now the dataset is tidy, but it's still not completely clean. To make it easy
to study time trends, let's combine and convert the `month` and `fiscal_year`
columns into a calendar date. You can use functions from the lubridate package
(Section \@ref(the-lubridate-package)) to do this. First paste the year and
month together and use the `my` function to parse them as dates:

```{r}
library(lubridate)

dates = my(paste(smart$Month, smart$fiscal_year))
dates
```

The SMART fiscal year extends from July to the following June and equals the
calendar year at the end of the period. So for observations from July to
December, the calendar year is the fiscal year minus 1. You can use indexing to
make this adjustment efficiently, and then append the dates to the data frame:

```{r}
jul2dec = month(dates) >= 7
dates[jul2dec] = dates[jul2dec] - period(1, "year")
smart$date = dates
head(smart)
```

As a final adjustment, you can use the `tolower` function to convert the column
names to lowercase, so that they're easier to use during analysis:

```{r}
names(smart) = tolower(names(smart))
smart
```

Now that the dataset is tidied and cleaned, it's straightforward to do things
like plot it as a time series:

```{r smart-plot}
library("ggplot2")

ggplot(smart) + aes(x = date, y = riders) + geom_line() +
  expand_limits(y = 0)
```

Notice the huge drop (more than 90%) in April of 2020 due to the COVID-19
pandemic!


### Without tidyr

This section shows how to pivot datasets without the help of the tidyr package.
In practice, we recommend that you use the package, but the examples here may
make it easier to understand what's actually happening when you pivot a
dataset.

#### Rows into Columns

The steps for pivoting `table2` wider are:

1. Subset rows to separate `cases` and `population` values.
2. Remove the `type` column from each.
3. Rename the `count` column to `cases` and `population`.
4. Merge the two subsets by matching `country` and `year`.

And the code is:

```{r}
# Step 1
cases = table2[table2$type == "cases", ]
pop = table2[table2$type == "population", ]
# Step 2
cases = cases[-3]
pop = pop[-3]
# Step 3
names(cases)[3] = "cases"
names(pop)[3] = "population"
# Step 4
merge(cases, pop)
```

#### Columns into Rows

The steps for pivoting `table4a` longer are:

1. Subset columns to separate `1999` and `2000` into two data frames.
2. Add a `year` column to each.
3. Rename the `1999` and `2000` columns to `cases`.
4. Stack the two data frames with `rbind`.

And the code is:

```{r}
# Step 1
df99 = table4a[-3]
df00 = table4a[-2]
# Step 2
df99$year = "1999"
df00$year = "2000"
# Step 3
names(df99)[2] = "cases"
names(df00)[2] = "cases"
# Step 4
rbind(df99, df00)
```



Iteration Strategies
--------------------

R is powerful tool for automating tasks that have repetitive steps. For
example, you can:

* Apply a transformation to an entire column of data.
* Compute distances between all pairs from a set of points.
* Read a large collection of files from disk in order to combine and analyze
  the data they contain.
* Simulate how a system evolves over time from a specific set of starting
  parameters.
* Scrape data from many pages of a website.

You can implement concise, efficient solutions for these kinds of tasks in R by
using **iteration**, which means repeating a computation many times. R provides
four different strategies for writing iterative code:

1. Vectorization, where a function is implicitly called on each element of a
   vector. See [this section][vectorization] of the R Basics for more details.
2. Apply functions, where a function is explicitly called on each element of a
   vector or array. See [this section][apply-functions] of the R Basics reader
   for more details.
3. Loops, where an expression is evaluated repeatedly until some condition is
   met.
4. Recursion, where a function calls itself.

[vectorization]: https://ucdavisdatalab.github.io/workshop_r_basics/data-structures.html#vectorization
[apply-functions]: https://ucdavisdatalab.github.io/workshop_r_basics/exploring-data.html#apply-functions

Vectorization is the most efficient and most concise iteration strategy, but
also the least flexible, because it only works with vectorized functions and
vectors. Apply functions are more flexible---they work with any function and
any data structure with elements---but less efficient and less concise. Loops
and recursion provide the most flexibility but are the least concise. In recent
versions of R, apply functions and loops are similar in terms of efficiency.
Recursion tends to be the least efficient iteration strategy in R.

The rest of this section explains how to write loops and how to choose which
iteration strategy to use. We assume you're already comfortable with
vectorization and have at least some familiarity with apply functions.


### For-loops

A **for-loop** evaluates an expression once for each element of a vector or
list. The `for` keyword creates a for-loop. The syntax is:

```{r, eval = FALSE}
for (I in DATA) {
  # Your code goes here
}
```

The variable `I` is called an **induction variable**. At the beginning of each
iteration, `I` is assigned the next element of `DATA`. The loop iterates once
for each element, unless a keyword instructs R to exit the loop early (more
about this in Section \@ref(break-next)). As with if-statements and functions,
the curly braces `{ }` are only required if the body contains multiple lines of
code. Here's a simple for-loop:

```{r}
for (i in 1:10)
  message("Hi from iteration  ", i)
```

When some or all of the iterations in a task depend on results from prior
iterations, loops tend to be the most appropriate iteration strategy. For
instance, loops are a good way to implement time-based simulations or compute
values in recursively defined sequences.

As a concrete example, suppose you want to compute the result of starting from
the value 1 and composing the sine function 100 times:

```{r}
result = 1
for (i in 1:100) {
  result = sin(result)
}

result
```

Unlike other iteration strategies, loops don't return a result automatically.
It's up to you to use variables to store any results you want to use later. If
you want to save a result from every iteration, you can use a vector or a list
indexed on the iteration number:

```{r}
n = 1 + 100
result = numeric(n)
result[1] = 1
for (i in 2:n) {
  result[i] = sin(result[i - 1])
}

plot(result)
```

Section \@ref(saving-multiple-results) explains this in more detail.

If the iterations in a task are not dependent, it's preferable to use
vectorization or apply functions instead of a loop. Vectorization is more
efficient, and apply functions are usually more concise.

In some cases, you can use vectorization to handle a task even if the
iterations are dependent. For example, you can use vectorized exponentiation
and the `sum` function to compute the sum of the cubes of many numbers:

```{r}
numbers = c(10, 3, 100, -5, 2, 10)
sum(numbers^3)
```


### While-loops

A **while-loop** runs a block of code repeatedly as long as some condition is
`TRUE`. The `while` keyword creates a while-loop. The syntax is:

```{r, eval = FALSE}
while (CONDITION) {
  # Your code goes here
}
```

The `CONDITION` should be a scalar logical value or an expression that returns
one. At the beginning of each iteration, R checks the `CONDITION` and exits the
loop if it's `FALSE`. As always, the curly braces `{ }` are only required if
the body contains multiple lines of code. Here's a simple while-loop:

```{r}
i = 0
while (i < 10) {
  i = i + 1
  message("Hello from iteration ", i)
}
```

Notice that this example does the same thing as the simple for-loop in Section
\@ref(for-loops), but requires 5 lines of code instead of 2. While-loops are a
generalization of for-loops, and only do the bare minimum necessary to iterate.
They tend to be most useful when you don't know how many iterations will be
necessary to complete a task.

As an example, suppose you want to add up the integers in order until the total
is greater than 50:

```{r}
total = 0
i = 1
while (total < 50) {
  total = total + i
  message("i is ", i, " total is ", total)
  i = i + 1
}

total
i
```


### Saving Multiple Results

Loops often produce a different result for each iteration. If you want to save
more than one result, there are a few things you must do.

First, set up an index vector. The index vector should usually correspond to
the positions of the elements in the data you want to process. The `seq_along`
function returns an index vector when passed a vector or list. For instance:

```{r}
numbers = c(-1, 21, 3, -8, 5)
index = seq_along(numbers)
```

The loop will iterate over the index rather than the input, so the induction
variable will track the current iteration number. On the first iteration, the
induction variable will be 1, on the second it will be 2, and so on. Then you
can use the induction variable and indexing to get the input for each
iteration.

Second, set up an empty output vector or list. This should usually also
correspond to the input, or one element longer (the extra element comes from
the initial value). R has several functions for creating vectors:

* `logical`, `integer`, `numeric`, `complex`, and `character` create an empty
  vector with a specific type and length

* `vector` creates an empty vector with a specific type and length

* `rep` creates a vector by repeating elements of some other vector

Empty vectors are filled with `FALSE`, `0`, or `""`, depending on the type of
the vector. Here are some examples:

```{r}
logical(3)
numeric(4)
rep(c(1, 2), 2)
```

Let's create an empty numeric vector congruent to the `numbers` vector:

```{r}
n = length(numbers)
result = numeric(n)
```

As with the input, you can use the induction variable and indexing to set the
output for each iteration.

Creating a vector or list in advance to store something, as we've just done, is
called **preallocation**. Preallocation is extremely important for efficiency
in loops. Avoid the temptation to use `c` or `append` to build up the output
bit by bit in each iteration.

Finally, write the loop, making sure to get the input and set the output. As an
example, this loop adds each element of `numbers` to a running total and
squares the new running total:

```{r}
for (i in index) {
  prev = if (i > 1) result[i - 1] else 0
  result[i] = (numbers[i] + prev)^2
}
result
```


### Break & Next

The `break` keyword causes a loop to immediately exit. It only makes sense to
use `break` inside of an if-statement.

For example, suppose you want to print each string in a vector, but stop at the
first missing value. You can do this with a for-loop and the `break` keyword:

```{r}
my_messages = c("Hi", "Hello", NA, "Goodbye")

for (msg in my_messages) {
  if (is.na(msg))
    break

  message(msg)
}
```

The `next` keyword causes a loop to immediately go to the next iteration. As
with `break`, it only makes sense to use `next` inside of an if-statement.

Let's modify the previous example so that missing values are skipped, but don't
cause printing to stop. Here's the code:

```{r}
for (msg in my_messages) {
  if (is.na(msg))
    next

  message(msg)
}
```

These keywords work with both for-loops and while-loops.


### Planning for Iteration

At first it may seem difficult to decide if and what kind of iteration to use.
Start by thinking about whether you need to do something over and over. If you
don't, then you probably don't need to use iteration. If you do, then try
iteration strategies in this order:

1. Vectorization
2. Apply functions
    * Try an apply function if iterations are independent.
3. Loops
    * Try a for-loop if some iterations depend on others.
    * Try a while-loop if the number of iterations is unknown.
4. Recursion (which isn't covered here)
    * Convenient for naturally recursive problems (like Fibonacci),
      but often there are faster solutions.


Start by writing the code for just one iteration. Make sure that code works;
it's easy to test code for one iteration.

When you have one iteration working, then try using the code with an iteration
strategy (you will have to make some small changes). If it doesn't work, try to
figure out which iteration is causing the problem. One way to do this is to use
`message` to print out information. Then try to write the code for the broken
iteration, get that iteration working, and repeat this whole process.


### Case Study: The Collatz Conjecture

The [Collatz Conjecture][collatz] is a conjecture in math that was introduced
in 1937 by Lothar Collatz and remains unproven today, despite being relatively
easy to explain. Here's a statement of the conjecture:

[collatz]: https://en.wikipedia.org/wiki/Collatz_conjecture

> Start from any positive integer. If the integer is even, divide by 2. If the
> integer is odd, multiply by 3 and add 1.
>
> If the result is not 1, repeat using the result as the new starting value.
>
> The result will always reach 1 eventually, regardless of the starting value.

The sequences of numbers this process generates are called **Collatz
sequences**. For instance, the Collatz sequence starting from 2 is `2, 1`. The
Collatz sequence starting from 12 is `12, 6, 3, 10, 5, 16, 8, 4, 2, 1`.

You can use iteration to compute the Collatz sequence for a given starting
value. Since each number in the sequence depends on the previous one, and since
the length of the sequence varies, a while-loop is the most appropriate
iteration strategy:

```{r}
n = 5
i = 0
while (n != 1) {
  i = i + 1
  if (n %% 2 == 0) {
    n = n / 2
  } else {
    n = 3 * n + 1
  }
  message(n, " ", appendLF = FALSE)
}
```

As of 2020, scientists have used computers to check the Collatz sequences for
every number up to approximately $2^{64}$. For more details about the Collatz
Conjecture, check out [this video][collatz-video].

[collatz-video]: https://www.youtube.com/watch?v=094y1Z2wpJg



### Case Study: U.S. Fruit Prices

The U.S. Department of Agriculture (USDA) Economic Research Service (ERS)
publishes data about consumer food prices. For instance, in 2018 they posted a
[dataset that estimates average retail prices for various fruits, vegetables,
and snack foods][fruit-veg-prices]. The estimates are formatted as a collection
of Excel files, one for each type of fruit or vegetable. In this case study,
you'll use iteration to get the estimated "fresh" price for all of the fruits
in the dataset that are sold fresh.

[fruit-veg-prices]: https://www.ers.usda.gov/data-products/fruit-and-vegetable-prices/

To get started, download the [zipped collection of fruit
spreadsheets][fruit-zip] and save it somewhere on your computer. Then unzip the
file with a zip program or R's own `unzip` function.

[fruit-zip]: https://www.ers.usda.gov/webdocs/DataFiles/51035/fruit%202013.zip?v=2437.6

The first sheet of each file contains a table with the name of the fruit and
prices sorted by how the fruit was prepared. You can see this for yourself if
you use a spreadsheet program to inspect some of the files.

In order to read the files into R, first get a vector of their names. You can
use the `list.files` function to list all of the files in a directory. If you
set `full.names = TRUE`, the function will return the absolute path to each
file:

```{r}
paths = list.files("data/fruit", full.names = TRUE)
paths
```

The files are in Excel format, which you can read with the `read_excel`
function from the [readxl][] package. First try reading one file and extracting
the fresh price:

```{r}
library("readxl")

prices = read_excel(paths[1])
```

The name of the fruit is the first word in the first column's name. The fresh
price appears in the row where the word in column 1 starts with `"Fresh"`. You
can use `str_which` from the stringr package (Section
\@ref(the-stringr-package)) to find and extract this row:

```{r}
library("stringr")

idx_fresh = str_which(prices[[1]], "^Fresh")
prices[idx_fresh, ]
```

The price and unit appear in column 2 and column 3.

Now generalize these steps by making a `read_fresh_price` function. The
function should accept a path as input and return a vector that contains the
fruit name, fresh price, and unit. Don't worry about cleaning up the fruit name
at this point---you can do that with a vectorized operation after combining the
data from all of the files. A few fruits don't have a fresh price, and the
function should return `NA` for the price and unit for those. Here's one way to
implement the `read_fresh_price` function:

```{r}
read_fresh_price = function(path) {
  prices = read_excel(path)

  # Get fruit name.
  fruit = names(prices)[[1]]

  # Find fresh price.
  idx = str_which(prices[[1]], "^Fresh")
  if (length(idx) > 0) {
    prices = prices[idx, ]
    c(fruit, prices[[2]], prices[[3]])
  } else {
    c(fruit, NA, NA)
  }
}
```

Test that the function returns the correct result for a few of the files:

```{r}
read_fresh_price(paths[[1]])
read_fresh_price(paths[[4]])
read_fresh_price(paths[[8]])
```

Now that the function is working, it's time to choose an iteration strategy.
The `read_fresh_price` function is not vectorized, so that strategy isn't
possible. Reading one file doesn't depend on reading any of the others, so
apply functions are the best strategy here. The `read_fresh_price` function
always returns a character vector with 3 elements, so you can use `sapply` to
process all of the files and get a matrix of results:

```{r}
all_prices = sapply(paths, read_fresh_price)

# Transpose, convert to a data frame, and set names for easy reading.
all_prices = t(all_prices)
all_prices = data.frame(all_prices)
rownames(all_prices) = NULL
colnames(all_prices) = c("fruit", "price", "unit")
all_prices
```

Finally, the last step is to remove the extra text from the fruit name. One way
to do this is with the `str_split_fixed` function from the stringr package.
There's an en dash `—` after each fruit name, which you can use for the split:

```{r}
fruit = str_split_fixed(all_prices$fruit, "—", 2)[, 1]
all_prices$fruit = fruit
all_prices
```

Now the data are ready for analysis. You could extend the reader function to
extract more of the data (e.g., dried and frozen prices), but the overall
process is fundamentally the same. Write the code to handle one file (one
step), generalize it to work on several, and then iterate.

For another example, see Liza Wood's [Real-world Function Writing
Mini-reader][liza-reader].

[liza-reader]: https://d-rug.github.io/realworld_functions_iteration/
